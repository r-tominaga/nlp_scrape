{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:05:58.209773Z",
     "start_time": "2019-04-01T01:05:57.033456Z"
    }
   },
   "outputs": [],
   "source": [
    "# import records\n",
    "# import sqlite3\n",
    "# from urllib.parse import urljoin, urldefrag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import csv\n",
    "import MeCab as mc\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree as tr\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "from gensim.models import word2vec, LdaModel\n",
    "from gensim import corpora\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_list = []\n",
    "for page in range(1,51):\n",
    "    url = f'https://tabelog.com/rstLst/steak/{page}/?Srt=D&SrtT=rt&sort_mode=1&sk=%E3%82%B9%E3%83%86%E3%83%BC%E3%82%AD&svt=1900&svps=2&select_sort_flg=1'\n",
    "    html_soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    for item in html_soup.find_all():\n",
    "        \n",
    "    restaurant_list.append(BeautifulSoup(requests.get(url).text, 'html.parser').find_all('a', class_='list-rst__rst-name-target cpy-rst-name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:06:30.531992Z",
     "start_time": "2019-03-28T10:06:30.527137Z"
    }
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "for restaurants in restaurant_list:\n",
    "    for restaurant in restaurants:\n",
    "        links.append(restaurant.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:16:22.756135Z",
     "start_time": "2019-04-11T06:16:22.752380Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tail = 'dtlrvwlst/?smp=0&lc=2&PG='\n",
    "rev_results = []\n",
    "\n",
    "# func = lambda review: [i.text for i in BeautifulSoup(requests.get('https://tabelog.com' + review).text, 'html.parser').find_all('div',class_='rvw-item__rvw-comment')]\n",
    "def get_review(review):\n",
    "    try:\n",
    "        res = requests.get('https://tabelog.com' + review, timeout=5)\n",
    "        return [i.text for i in BeautifulSoup(res.text, 'html.parser').find_all('div',class_='rvw-item__rvw-comment')]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    if n_links % 50 == 0:\n",
    "        np.savetxt(\"./texts/addition_700.csv\", rev_results, fmt=\"%s\", delimiter=\",\")\n",
    "    iteration = 1\n",
    "    while(True):\n",
    "        url = link + tail + str(iteration)\n",
    "        try:\n",
    "            r = requests.get(url,timeout=5)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            iteration += 1\n",
    "            review_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            review_urls =  [item.get('href') for item in review_soup.find_all('a', class_='rvw-simple-item__title-target')]\n",
    "            rev_results.append(\n",
    "                Parallel(n_jobs=4)([\n",
    "                    delayed(get_review)(i) for i in review_urls\n",
    "                ])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:01:38.763937Z",
     "start_time": "2019-03-28T12:01:38.761045Z"
    }
   },
   "outputs": [],
   "source": [
    "# レストラン名と評価のDict作成\n",
    "# restaurant_names = [item.string for item in html_soup.find_all('a', class_='list-rst__rst-name-target cpy-rst-name')]\n",
    "# ratings = [item.string for item in html_soup.find_all('span', class_='c-rating__val c-rating__val--strong list-rst__rating-val')]\n",
    "# for k,v in zip(restaurant_names, ratings):\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:06:49.361423Z",
     "start_time": "2019-04-01T01:06:48.466197Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('./texts/', 'wb')\n",
    "# pickle.dump(rev_results, f)\n",
    "# f.close()\n",
    "\n",
    "# ロード時コマンド\n",
    "# results = pickle.load(open('./texts/sample_steak_corpus.csv', 'rb'))\n",
    "\n",
    "csv_file = open(\"./texts/restaurant_reviews.csv\", \"r\", errors=\"\", newline=\"\" )\n",
    "\n",
    "#リスト形式\n",
    "f = csv.reader(csv_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "results= [row for row in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:06:49.528551Z",
     "start_time": "2019-04-01T01:06:49.363876Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = open(\"./texts/addition_700.csv\", \"r\", errors=\"\", newline=\"\" )\n",
    "#リスト形式\n",
    "f = csv.reader(csv_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "results2= [row for row in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:06:51.195282Z",
     "start_time": "2019-04-01T01:06:51.191709Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_results = results + results2[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:07:07.435180Z",
     "start_time": "2019-04-04T09:07:07.425176Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(texts):\n",
    "    t = mc.Tagger(\"-Ochasen\")\n",
    "    # 辞書ロードうまくいかない\n",
    "    #t = mc.Tagger( '-Owakati -d /Users/tominagaryota/mecab-ipadic-neologd')\n",
    "    t.parse('')\n",
    "    output = []\n",
    "    node =  t.parseToNode(texts)  \n",
    "    while node:\n",
    "        if node.surface != \"\":  # ヘッダとフッタを除外\n",
    "            word_type = node.feature.split(\",\")[0]\n",
    "            output.append(node.surface)\n",
    "        node = node.next\n",
    "        if node is None:\n",
    "            break\n",
    "    return output\n",
    "\n",
    "def mecab_analysis(texts):\n",
    "    t = mc.Tagger(\"-Ochasen\")\n",
    "    # 辞書ロードうまくいかない\n",
    "    #t = mc.Tagger( '-Owakati -d /Users/tominagaryota/mecab-ipadic-neologd')\n",
    "    t.parse('')\n",
    "    output = []\n",
    "    node =  t.parseToNode(texts)  \n",
    "    while node:\n",
    "        if node.surface != \"\":  # ヘッダとフッタを除外\n",
    "            word_type = node.feature.split(\",\")[0]\n",
    "            if word_type in ['名詞', '形容詞']:\n",
    "                output.append(node.surface)\n",
    "            #output.append(node.surface)\n",
    "        node = node.next\n",
    "        if node is None:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T08:16:07.660618Z",
     "start_time": "2019-03-26T08:08:03.217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sklearnでのLDA。使わないかも\n",
    "\n",
    "# import mglearn\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "# # mecab_analysisを使用\n",
    "# X = vect.fit_transform(mecab_analysis([r[0] for r in rev_results[0]]))\n",
    "# lda = LatentDirichletAllocation(n_topics=5, learning_method=\"batch\", max_iter=25, random_state=0)\n",
    "# document_topics = lda.fit_transform(X)\n",
    "\n",
    "# sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# feature_names = np.array(vect.get_feature_names())\n",
    "# mglearn.tools.print_topics(topics=range(5), feature_names=feature_names, \n",
    "#                            sorting=sorting, topics_per_chunk=1, n_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T08:16:07.662270Z",
     "start_time": "2019-03-26T08:08:21.737Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "# N_TOPICS = 6\n",
    "# # mecab_analysisを使用\n",
    "# X = vect.fit_transform(mecab_analysis([r[0] for r in rev_results[1]]))\n",
    "# lda = LatentDirichletAllocation(n_topics=N_TOPICS, learning_method=\"batch\", max_iter=25, random_state=0)\n",
    "# document_topics = lda.fit_transform(X)\n",
    "\n",
    "# sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# feature_names = np.array(vect.get_feature_names())\n",
    "# mglearn.tools.print_topics(topics=range(N_TOPICS), feature_names=feature_names, \n",
    "#                            sorting=sorting, topics_per_chunk=1, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:08:01.614487Z",
     "start_time": "2019-04-01T01:08:01.586564Z"
    }
   },
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "for rvws in rev_results:\n",
    "    for r in rvws:\n",
    "        all_reviews.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:08:04.502706Z",
     "start_time": "2019-04-01T01:08:04.498225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117619"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:49:37.715926Z",
     "start_time": "2019-04-01T01:49:35.611524Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = open(\"./texts/wakati.csv\", \"r\", errors=\"\", newline=\"\" )\n",
    "\n",
    "#リスト形式\n",
    "f = csv.reader(csv_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "wakati_text = [row for row in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:49:43.440838Z",
     "start_time": "2019-04-01T01:49:37.717944Z"
    }
   },
   "outputs": [],
   "source": [
    "new_wakati = []\n",
    "for text in wakati_text:\n",
    "    striped =  [t.strip(\"['n'\").strip(\"'n']\").strip(\"'\") for t in text]\n",
    "    new_wakati.append(striped)\n",
    "wakati_text = new_wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:17:03.624303Z",
     "start_time": "2019-04-11T06:17:03.621499Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = word2vec.Word2Vec(new_wakati, size=1000,min_count=5,window=5,iter=100,workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-29T04:55:19.194Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('ステーキ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:07:25.199336Z",
     "start_time": "2019-03-26T02:07:25.116357Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルのセーブ\n",
    "model.save(\"./model/sample_steak_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:03:25.892367Z",
     "start_time": "2019-03-26T02:03:25.807674Z"
    }
   },
   "outputs": [],
   "source": [
    "#np.savetxt(\"./texts/sample_steak_corpus.csv\", wakati_text, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:08:14.266491Z",
     "start_time": "2019-03-26T02:08:14.260059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagaryota/.anyenv/envs/pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('エビフライ', 0.6223527789115906),\n",
       " ('肉汁', 0.5919903516769409),\n",
       " ('デミグラスソース', 0.5824424624443054),\n",
       " ('肉感', 0.5666879415512085),\n",
       " ('フライ', 0.5600730180740356),\n",
       " ('俵', 0.5347720384597778),\n",
       " ('パクリ', 0.5159295797348022),\n",
       " ('デミ', 0.5084888339042664),\n",
       " ('ジュワー', 0.5022367238998413),\n",
       " ('箸', 0.49991917610168457)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[u\"挽肉\",u\"ハンバーグ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:50:58.567365Z",
     "start_time": "2019-04-01T01:50:47.580850Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(wakati_text)\n",
    "dictionary.save_as_text('./texts/text.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:51:19.677419Z",
     "start_time": "2019-04-01T01:51:00.012067Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in wakati_text]\n",
    "corpora.MmCorpus.serialize('./texts/text.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:51:33.322099Z",
     "start_time": "2019-04-01T01:51:30.874838Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:51:44.288505Z",
     "start_time": "2019-04-01T01:51:40.739285Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./model/corpus_tfidf.dump', mode='wb') as f:\n",
    "    pickle.dump(corpus_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:20:50.776387Z",
     "start_time": "2019-04-01T01:52:48.672935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_lda = gensim.models.LdaMulticore(corpus=corpus_tfidf, id2word=dictionary,\n",
    "                             num_topics=10, minimum_probability=0.001,\n",
    "                             passes=20, eval_every=0, chunksize=1000,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:22:19.858707Z",
     "start_time": "2019-04-01T02:22:19.727658Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_lda.save('./model/tfidf_lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:26:54.575570Z",
     "start_time": "2019-04-01T01:26:54.512683Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('model/tfidf_lda.model', mode='rb') as f:\n",
    "#     tfidf_lda = pickle.load(f)\n",
    "tfidf_lda = gensim.models.LdaModel.load('./model/tfidf_lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:22:25.281922Z",
     "start_time": "2019-04-01T02:22:25.237603Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpc_0: 0.006*\"希少\" + 0.004*\"おにぎり\" + 0.004*\"松阪\" + 0.003*\"テキ\" + 0.003*\"浜松\" + 0.003*\"表参道\" +...\n",
      "tpc_1: 0.009*\"u3\" + 0.008*\"000\" + 0.003*\"お肉\" + 0.003*\"の\" + 0.003*\"０\" + 0.003*\"店\" + 0.00...\n",
      "tpc_2: 0.011*\"000\" + 0.011*\"1\" + 0.007*\"500円\" + 0.007*\"u3\" + 0.007*\"円\" + 0.006*\"2\" + 0....\n",
      "tpc_3: 0.016*\"\" + 0.006*\"うし\" + 0.004*\"バンビーナ\" + 0.003*\"フライデーズ\" + 0.003*\"極み\" + 0.003*\"割烹\"...\n",
      "tpc_4: 0.010*\"ハンバーグ\" + 0.006*\"ステーキ\" + 0.004*\"０\" + 0.004*\"肉\" + 0.004*\"ソース\" + 0.004*\"ライス\"...\n",
      "tpc_5: 0.012*\"xa\" + 0.010*\"トンテキ\" + 0.009*\"0\" + 0.007*\"050\" + 0.006*\"スペアリブ\" + 0.003*\"108...\n",
      "tpc_6: 0.012*\"牛タン\" + 0.004*\"焼肉店\" + 0.003*\"西麻布\" + 0.003*\"卵黄\" + 0.002*\"網焼き\" + 0.002*\"センス\"...\n",
      "tpc_7: 0.005*\"280円\" + 0.005*\"ミスジ\" + 0.003*\"トニーローマ\" + 0.003*\"ビアホール\" + 0.003*\"白子\" + 0.003...\n",
      "tpc_8: 0.006*\"げんこつハンバーグ\" + 0.005*\"ラム\" + 0.004*\"海鮮\" + 0.003*\"新橋\" + 0.003*\"開放\" + 0.003*\"ハ...\n",
      "tpc_9: 0.003*\"レバ刺し\" + 0.003*\"ヘレ\" + 0.002*\"フランス料理\" + 0.002*\"ローストチキン\" + 0.002*\"伊勢\" + 0.00...\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('tpc_{0}: {1}'.format(i, tfidf_lda.print_topic(i)[0:80]+'...'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T04:49:05.202277Z",
     "start_time": "2019-03-26T04:49:05.199743Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF処理をしないLDA\n",
    "# raw_lda = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary,\n",
    "#                              num_topics=10, minimum_probability=0.001,\n",
    "#                              passes=20, eval_every=0, chunksize=10000,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:32:25.902375Z",
     "start_time": "2019-04-01T01:32:25.899081Z"
    }
   },
   "outputs": [],
   "source": [
    "temp ='天下茶屋でランチの機会があればその都度狙っていたこちらの超人気洋食店。売りは一頭買いするらしい黒毛を使ったお肉料理という触れ込みですわ。中でも黒バーグいうハンバーグがメインらしい情報は温めてました＾＾場所は天下茶屋駅から数分です。立ち飲み店がしのぎを削るサイドではない場所でテニススクールのあるあたり。健全でよろしおす(笑)。いうても、平日のランチなんで・・・とはいっても、一番安いハンバーグのランチでも1500円となかなかの高額です。それ相応のレベルは求められるはず。この日は1時前でしたが満席でした。かなり評判がいいみたいです。□黒バーグとカキフライ（1800円）○　エビフライなどの海鮮のフライも人気とのことで、カキフライを＾＾ｖ　まず、肝心の黒バーグですが、うん、僕は美味しいと思いました。　特徴的はなのは、スパイス含め、お肉に味がしっかりついてることと、　牛肉の香りも歯ごたえもあるけど、全然しつこくなくて、なおかつ、　デミグラスソースもあっさりしていて食べやすいです。脂こってりの　濃厚ソースみたいなハンバーグが好きな人には向かないけど、ただ、　　そうだとして、焼きも含めてくいしんぼー山中さんクラスのハンバーグが　といえばあまりに普通かなあ。デミグラスソースも2000円出すにしては。　カキフライは、大きさ重視です。タルタルはおいしいけど、揚げる牡蠣と　してはどうなんだろういう印象。このあたりが、叩き上げのシェフの洋食店　とは一線を画するといったところなのかな。　コンソメ・ご飯は普通にいけました＾＾確かに、この地域で見れば突出してるかもしれないですけど、作法・値段は大阪市内水準です。美味しいのは美味しいんですけどね＾＾ｖ昼に天下茶屋に行かれた際には是非。ご参考まで♪'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:22:56.466074Z",
     "start_time": "2019-04-01T02:22:56.238959Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.016632209),\n",
       " (1, 0.46797404),\n",
       " (4, 0.48619828),\n",
       " (6, 0.0087923305),\n",
       " (8, 0.016394185)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = dictionary.doc2bow(mecab_analysis(temp))\n",
    "tfidf_lda[vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:27:45.291603Z",
     "start_time": "2019-04-01T01:27:45.286644Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA分類したベクトルをnumpyベクトル化\n",
    "def convert_to_np_vec(vec):\n",
    "    n_v = np.zeros(50)\n",
    "    for v in vec:\n",
    "        n_v[v[0]] = v[1]\n",
    "    return n_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:27:45.962507Z",
     "start_time": "2019-04-01T01:27:45.958389Z"
    }
   },
   "outputs": [],
   "source": [
    "# コサイン類似度を計算\n",
    "def cos_sim(v1,v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:23:05.098778Z",
     "start_time": "2019-04-01T02:23:04.966986Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = dictionary.doc2bow(mecab_analysis(rvws[10]))\n",
    "vec2 = dictionary.doc2bow(mecab_analysis(rvws[14]))\n",
    "vec1 = convert_to_np_vec(tfidf_lda[vec])\n",
    "vec2 = convert_to_np_vec(tfidf_lda[vec2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T08:35:50.344560Z",
     "start_time": "2019-04-02T08:35:50.305871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7385122087001152"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:17:13.380752Z",
     "start_time": "2019-04-11T06:17:13.377108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = word2vec.Text8Corpus(\"./wiki/wiki_wakati.txt\")\n",
    "model =  word2vec.Word2Vec(data, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T02:16:28.663459Z",
     "start_time": "2019-04-03T02:16:27.013825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagaryota/.anyenv/envs/pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('単純', 0.7389036417007446),\n",
       " ('低級', 0.7317520380020142),\n",
       " ('邪悪', 0.6965624690055847),\n",
       " ('不完全', 0.6847642660140991),\n",
       " ('無機質', 0.6803640127182007),\n",
       " ('鋭敏', 0.6790430545806885),\n",
       " ('複雑', 0.6772918701171875),\n",
       " ('微細', 0.676936686038971),\n",
       " ('獰猛', 0.6749590635299683),\n",
       " ('醜悪', 0.6736862063407898)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['純粋','悪'],negative = ['正義'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T08:38:38.100304Z",
     "start_time": "2019-04-03T08:38:37.998739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagaryota/.anyenv/envs/pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('つながり', 0.9462962746620178),\n",
       " ('関わり', 0.8196489810943604),\n",
       " ('結びつき', 0.8081238269805908),\n",
       " ('結び付き', 0.7764103412628174),\n",
       " ('関係性', 0.7676485776901245),\n",
       " ('かかわり', 0.7665578126907349),\n",
       " ('信頼関係', 0.7625255584716797),\n",
       " ('関連性', 0.7572896480560303),\n",
       " ('関係', 0.7516380548477173),\n",
       " ('係わり', 0.7378243207931519)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['繋がり'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T08:39:00.869928Z",
     "start_time": "2019-04-03T08:39:00.776948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagaryota/.anyenv/envs/pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('徳', 0.6933034658432007),\n",
       " ('義', 0.6846591234207153),\n",
       " ('吾', 0.681052029132843),\n",
       " ('慮', 0.6785447597503662),\n",
       " ('聚', 0.6748489737510681),\n",
       " ('鑑', 0.6738952994346619),\n",
       " ('祇', 0.6736379861831665),\n",
       " ('澄', 0.6709215641021729),\n",
       " ('為政', 0.6708956956863403),\n",
       " ('箕', 0.6702141761779785)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['志'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T08:40:56.458183Z",
     "start_time": "2019-04-03T08:40:56.365693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagaryota/.anyenv/envs/pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('内蔵式ギアボックス', 0.42438018321990967),\n",
       " ('カスケードトンネル', 0.41108304262161255),\n",
       " ('R6RS', 0.40986916422843933),\n",
       " ('ドルトムント市電', 0.4021916091442108),\n",
       " ('バヨネットマウント', 0.3970351815223694),\n",
       " ('シンドラー社', 0.39160555601119995),\n",
       " ('スタジオ・ハーフ・アイ', 0.38931676745414734),\n",
       " ('メモリアロケータ', 0.3883298635482788),\n",
       " ('シビアコンディション', 0.3876619040966034),\n",
       " ('ASELSAN', 0.38623949885368347)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative = ['感謝'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T08:46:35.004963Z",
     "start_time": "2019-04-04T08:46:34.924729Z"
    }
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:56:42.427152Z",
     "start_time": "2019-04-04T09:56:41.264179Z"
    }
   },
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "for rvws in rev_results:\n",
    "    for r in rvws:\n",
    "        all_reviews.append(r.strip(\"[['\\\\n\\\\n\").strip(\" \").strip(\"\\n\\\\n']\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:21:21.183821Z",
     "start_time": "2019-04-04T10:12:59.237825Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized = [mc.Tagger('-Owakati').parse(review) for review in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:21:21.314660Z",
     "start_time": "2019-04-04T10:21:21.186485Z"
    }
   },
   "outputs": [],
   "source": [
    "text = ''.join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:05:24.844562Z",
     "start_time": "2019-04-04T10:05:24.832644Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_text(t):\n",
    "    t = t.replace('　', ' ')  # Full width spaces\n",
    "    # t = re.sub(r'([。．！？…]+)', r'\\1\\n', t)  # \\n after ！？\n",
    "    t = re.sub(r'(.+。) (.+。)', r'\\1 \\2\\n', t)\n",
    "    t = re.sub(r'\\n +', '\\n', t)  # Spaces\n",
    "    t = re.sub(r'([。．！？…])\\n」', r'\\1」 \\n', t)  # \\n before 」\n",
    "    t = re.sub(r'\\n +', '\\n', t)  # Spaces\n",
    "    t = re.sub(r'\\n+', r'\\n', t).rstrip('\\n')  # Empty lines\n",
    "    t = re.sub(r'\\n +', '\\n', t)  # Spaces\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:36:32.210630Z",
     "start_time": "2019-04-04T10:35:02.308358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京 で 食べる と 生肉 の 塩 、 コショー 、 ナツメグ など の 柑橘系 の 爽やか な ハイボール で 乾杯 ！ 空腹 を 埋める 事 が 少ない 駅前通 に ある 、 知ら ない もっと いい のに 。 。 ＊ デザート ＊ カウンター から 調理 が 1人 で 食べ てる 方 も 、 800円 前後 の 方 に 連れ て 行く トコ じゃ なかっ た の だ 。 これ は ソース より 塩 っけ が 強く なり 、 移動 。 20時 くらい に 4 名 様 へ \\ u3 000 ええ 肉 出す 店 が ある 。 値段 は とり ず らく 温厚 な 連れ が 、 サービス 別 ） 。 次に カット 場 で 味付け さ れ て いる だけ で クセ が 強い です 。 大きく 切っ て ださ れる 品々 でし た 。 見た目 や 色 々 頂き まし た ！ 色々 調べ た 結果 、 お つけもの から 提供 まで の 予約 を し て くれ た の だ が 、 ハンバーグ の 特徴 と 思わ れる 付近 の 席 と の 相性 良く て 、 セット の 方 が ウリ の 焼き アイスクリーム は 美味しく 感じ ます 。 全体的 に 油 が 飛び散る ので 紙 エプロン 用意 し て くれる が また れ て ます が 、 立地 と 駐車場 に 駐車 。 4 . 3 くらい しか 食べ ない こと に なる もの 今日 は どこ も 並ん でる 人達 が 多い 、 260g じゃ 物足りない か な と 。 味 変 として お 邪魔 し た 白米 に 吸わ せ た 方 が ほとんど い ない 。 ひたすら パン を 食べ て みる と 、 和牛 と オーストラリア 牛 が ええ っていう 訳 の わから ない の が 基本 です 。 女子会 や カップル など 客層 は やや 重かっ た 。 しかし 、 その 席 へ やっぱり 気 に なる 美味し さ で 驚き 。 これ が 実に よく このまま ご飯 に とても おすすめ です 。\n",
      "うまい 生ビール を 注文 。 こちら の 方 から の そうだ ステーキ 行こう ！ 」 て 感じ た 。 うす 暗く て 写真 を 載せ た 「 そばめし 」 も おすすめ し ます 。 \\ u3 000 思わず 「 今日 は スターゲイト の ご 注文 は デミグラスハンバーグ を 頼み まし た 。 酒の肴 が あまり よく なく 、 脇 を 固め 、 味噌汁 、 ごはん を 食べ た いって くらい 柔らか で 美味しかっ た です 。 \\ u3 000４ \\ u3 000 やみつき メンチ ＠ 100円 ■ デザート ● 実演 最大 1000人 分 の 予算 の 領域 と 言え ば ステーキ 。 定期的 に 行き たい です な （ テーブル の 人 は 、 ご飯 （ １ ８ ９ ０ 円 \\ u3 000 \\ u3 000 後 で お 馴染み 客 が お 替り し たい と 思い まし た 。 自家製 の アイスクリーム コーヒー （ 2 名 。 ここ で いい と 感じ させ ない 手法 が アメリカ っぽい お 店 なんて 、 期待 以上 に カッコ よく ない 気 が 。 私 が 訪問 し まし た 。 席 は ２ 階 も 結構 埋まっ て しまう … 。\n",
      "500 の コース です と 伝える と 店員 さん 3 名 ほど で も 特に 印象 に は おいしい 。 私 が 着い た の です が 、 繊細 な 味 、 ミディアムレア で ニンニク ドレッシング を 醤油 ベース で 少し 待ち まし た 。 追加 料金 で あっ て 、 目 の 前 で 調理 し て いる メイン は ・ ・ これ は 多分 、 合わ ない 。 ここ は 私 の 大好き な 万世 で ニンニク を いっぱい 飲み まし た 。 ランチ に いっ て ー 丼 は 松阪牛 の 正式 な 神戸牛 の カルパッチョ より ソース の 様 な 感じ ・ ・ ・ ・ ・ ・ ・ 難しい です が ドレッシング が あっ たり 、 お 味 も 良く 準備 できる と の 会食 で この 値段 で は ない です が 、 ぎこち ない 空間 が おしゃれ です 。 なかなか 美味 です 。 関東 トンカツ サンド と ハンバーグ は 肉汁 たっぷり 。 味 は 思っ て い て 常に 並ん で 待っ て 、 周り の お客さん が おり 、 一品 で 、 レモン が ステーキ に 使用 し 、 返事 が 帰っ た か ？ 。 。 しかし 、 空い た カウンター に 着席 し おしぼり を 出す と 、 アトレ WEST の ランチ は 値打ち が あり まし た 。 ただ 、 私 が 食べ たい モード 全開 で とても 美味しかっ た ので フルーツ の 味 が かわっ たら また 行こ う と 、 ハンバーグ を 見 ます 。 ましてや 喫煙 必須 の お 値段 で 頼め ば 食べ放題 の コース で 伺い まし た 。 あの 肉質 で \\ u3 000 私 の テーブル は ものすごく ボリューム 満点 ！ からあげ & 生姜焼き 、 かに クリームコロッケ 、 これ 、 一人 でも 気軽 に 食べ た 中 で 勝ち ます 。 イタリアンアイスピーチティー に し て あり ます 。 待つ こと しばし ， 提供 さ れ た ので 、 空間 が 広がっ て 、 島 野菜サラダ 」 ・ ・ ・ ・ ・ ● お 店 を 変え て オシャレ な 場所 に ある お 店 は Ｍ 嬢 ご 注文 。 発券 し て 行き たい お 店 が 「 ヨシカミ 」 を 超え まし た 。 店内 は 掃除 が あまり 慣れ て いる とき に 訪れる に は 伺え なかっ た こと が ない ・ ・ ・ ・ と 考える と \\ u3 000 肉 バル の よう な 雰囲気 店内 は ほぼ 同じ 感想 です 。\n",
      "仕事 の 打合せ へ １ 筋 歩い た ところ \\ u3 000 洋食屋 な ので 、 お腹 は いっぱい でし た 。 戴い た アップルパイ みたい な 外観 です が 、 ここ は 美味しい の 、 お 邪魔 し ます 。 どうも 、 こんばんみ 。 新橋 SL広場 から すぐ に 来 た ので 、 きっと 店 に ありがち な 上品 さ は 無い でしょ う ね 。 白 を オーダー 、 パン など が あり ます が 、 入っ て ます 。 その 割 に は たまらない お 店 です から ランチ は 、 タン 特有 の ヘーゼルナッツ の ドーム の 姉妹 店 、 10時 まで バタバタ し て いる お 店 決定 権 が 別 に 供 さ れる 。\n",
      "760 という お 父 し ゃんとおばしゃんには 良かっ た です 。 ランチ 場所 は 秋葉原駅 から なら 徒歩 １ ０ ０ 円 で 頂け 一 人 でも 飲み放題 だっ た の です 。 そして 間も無く 豪快 な 気分 に なれ ます 。 正直 、 ご飯 が 美味しく 感じ た 。 \\ u3 000 デザート これ に 勝る と も 思う 。\n"
     ]
    }
   ],
   "source": [
    "text_model = markovify.NewlineText(format_text(text), 2)\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:07:23.742468Z",
     "start_time": "2019-04-04T10:07:23.465938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食べる事ができました！美味しかったー！！ごちそうさまでした。\n",
      "800円通常メニュー黒毛和牛100％フレッシュハンバーグ300ｇ2\n",
      "以前からずーっと行きたいリストNO1だったミート矢澤さんに出掛けました。黒毛和牛100%デミグラスチーズハンバーグ2\n",
      "500円黒毛和牛100%デミグラスチーズハンバーグ1\n",
      "800円通常メニュー黒毛和牛100％フレッシュハンバーグ300ｇ2\n",
      "150円黒毛和牛100%フレッシュハンバーグ\\u3000桜が咲くころは\\u3000Large2\n",
      "u3000\\u3000Regular3\n",
      "五反田でお昼過ぎに商談が終わったのでハンバーグを食べさせてくれ。本当の評価はそれからだ。\n",
      "800円通常メニュー黒毛和牛100％フレッシュハンバーグを注文です。黒毛和牛100%デミグラスチーズハンバーグ2\n",
      "200円黒毛和牛100%フレッシュハンバーグを注文して¥4\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sentence = text_model.make_short_sentence(100, 20, tries=100).replace(' ', '')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
